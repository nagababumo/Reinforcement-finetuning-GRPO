{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a458613f",
   "metadata": {},
   "source": [
    "# Lesson 3: Can a large language model master Wordle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9d6cb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> 💻 &nbsp; <b>Access <code>requirements.txt</code>  file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> 📒 &nbsp; For more help, please see the <em>\"Appendix – Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6720b3",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> 🚨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e083d05",
   "metadata": {},
   "source": [
    "Start by load dependencies and setting up the Predibase client, which you'll use to call both base and finetuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed682fe5-6329-42da-9e5e-ce019aba8c77",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client to query Qwen2.5 7B Instruct on Predibase using the OpenAI client.\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.environ[\"PREDIBASE_MODEL_QWEN_URL\"], # Qwen 2.5 7B Instruct\n",
    "    api_key=os.environ[\"PREDIBASE_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478e01c",
   "metadata": {},
   "source": [
    "Load the model tokenizer from Hugging face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84308d60-a3b3-45c2-8918-ddde153a9202",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88853c5aed645e7a7115def8e95b9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768a6da636224c979a16a3747e908389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a911205d64c43719d7b29e5d62ae33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd53d7ed8e84939a7e4854288718250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5c160",
   "metadata": {},
   "source": [
    "## Setting up prompts to play Wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4482b0-cc4e-4b77-9a04-b3401a824c9d",
   "metadata": {
    "height": 506
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are playing Wordle, a word-guessing game.\n",
    "\n",
    "### Game Rules:\n",
    "- You have **6 tries** to guess a secret **5-letter** word.\n",
    "- Each guess must be a valid **5-letter English word**.\n",
    "- After each guess, you will receive feedback indicating how close \n",
    "your guess was.\n",
    "\n",
    "### Feedback Format:\n",
    "Each letter in your guess will receive one of three symbols:\n",
    "1. ✓ : The letter is in the word and in the CORRECT position.\n",
    "2. - : The letter is in the word but in the WRONG position.\n",
    "3. x : The letter is NOT in the word.\n",
    "\n",
    "### Example:\n",
    "Secret Word: BRISK\n",
    "\n",
    "Guess 1: STORM → Feedback: S(-) T(x) O(x) R(-) M(x)\n",
    "Guess 2: BRAVE → Feedback: B(✓) R(✓) A(x) V(x) E(x)\n",
    "Guess 3: BRISK → Feedback: B(✓) R(✓) I(✓) S(✓) K(✓)\n",
    "\n",
    "### Response Format:\n",
    "Think through the problem and feedback step by step. Make sure to \n",
    "first add your step by step thought process within <think> </think> \n",
    "tags. Then, return your guessed word in the following format: \n",
    "<guess> guessed-word </guess>.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc165b8b-f396-471d-9d84-16d964ea0cb2",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class LetterFeedback(Enum):\n",
    "    CORRECT = \"✓\"\n",
    "    WRONG_POS = \"-\"\n",
    "    WRONG_LETTER = \"x\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GuessWithFeedback:\n",
    "    guess: str\n",
    "    feedback: List[LetterFeedback]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Returns a readable string showing the guess alongside\n",
    "        its letter-by-letter feedback.\"\"\"\n",
    "        feedback_str = \" \".join(f\"{letter}({fb.value})\" for letter, fb in zip(self.guess, self.feedback))\n",
    "        return f\"{self.guess} → Feedback: {feedback_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ac5b76-c834-4674-9e0b-484a2219ec69",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "def render_user_prompt(past_guesses: List[GuessWithFeedback]) -> str:\n",
    "    \"\"\"Creates a user-facing prompt that includes past guesses \n",
    "    and their feedback.\"\"\"\n",
    "    prompt = \"Make a new 5-letter word guess.\"\n",
    "    if past_guesses:\n",
    "        prompt += \"\\n\\nHere is some previous feedback:\"\n",
    "        for i, guess in enumerate(past_guesses):\n",
    "            prompt += f\"\\nGuess {i+1}: {guess}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2386034-ae0b-4a16-ad38-83b01982d97b",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "def render_prompt(past_guesses: List[GuessWithFeedback]):\n",
    "    \"\"\"Formats a full chat prompt using a system message, user \n",
    "    prompt, and assistant preamble to start the model's \n",
    "    step-by-step reasoning.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": render_user_prompt(past_guesses)\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, continue_final_message=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8aca9bc-a37f-474a-aa43-deedea322097",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "def generate_stream(prompt: str, adapter_id: str = \"\") -> str:\n",
    "    \"\"\"Streams a model-generated response from a prompt in \n",
    "    real-time and prints it as it arrives.\"\"\"\n",
    "    response = client.completions.create(\n",
    "        model=adapter_id,\n",
    "        prompt=prompt,\n",
    "        # Produce deterministic responses for evaluation\n",
    "        temperature=0.0, \n",
    "        max_tokens=2048,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    completion = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].text is not None:\n",
    "            content = chunk.choices[0].text\n",
    "            print(content, end=\"\", flush=True)\n",
    "            completion += content\n",
    "    print()\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5c34e",
   "metadata": {},
   "source": [
    "## Play a single turn of Wordle\n",
    "\n",
    "Start by setting up the prompt with feedback for two prior guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4ea278-1bd1-433d-9f7c-e73da319ad3d",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "You are playing Wordle, a word-guessing game.\n",
      "\n",
      "### Game Rules:\n",
      "- You have **6 tries** to guess a secret **5-letter** word.\n",
      "- Each guess must be a valid **5-letter English word**.\n",
      "- After each guess, you will receive feedback indicating how close \n",
      "your guess was.\n",
      "\n",
      "### Feedback Format:\n",
      "Each letter in your guess will receive one of three symbols:\n",
      "1. ✓ : The letter is in the word and in the CORRECT position.\n",
      "2. - : The letter is in the word but in the WRONG position.\n",
      "3. x : The letter is NOT in the word.\n",
      "\n",
      "### Example:\n",
      "Secret Word: BRISK\n",
      "\n",
      "Guess 1: STORM → Feedback: S(-) T(x) O(x) R(-) M(x)\n",
      "Guess 2: BRAVE → Feedback: B(✓) R(✓) A(x) V(x) E(x)\n",
      "Guess 3: BRISK → Feedback: B(✓) R(✓) I(✓) S(✓) K(✓)\n",
      "\n",
      "### Response Format:\n",
      "Think through the problem and feedback step by step. Make sure to \n",
      "first add your step by step thought process within <think> </think> \n",
      "tags. Then, return your guessed word in the following format: \n",
      "<guess> guessed-word </guess>.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Make a new 5-letter word guess.\n",
      "\n",
      "Here is some previous feedback:\n",
      "Guess 1: CRANE → Feedback: C(✓) R(✓) A(✓) N(x) E(x)\n",
      "Guess 2: CRASH → Feedback: C(✓) R(✓) A(✓) S(x) H(x)<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let me solve this step by step.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "secret_word = \"CRAFT\"\n",
    "\n",
    "past_guesses = [\n",
    "    GuessWithFeedback(\n",
    "        \"CRANE\", [\n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.WRONG_LETTER, \n",
    "            LetterFeedback.WRONG_LETTER,\n",
    "        ]),\n",
    "    GuessWithFeedback(\n",
    "        \"CRASH\", [\n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.CORRECT, \n",
    "            LetterFeedback.WRONG_LETTER, \n",
    "            LetterFeedback.WRONG_LETTER,\n",
    "        ]),\n",
    "]\n",
    "\n",
    "prompt = render_prompt(past_guesses)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c6318",
   "metadata": {},
   "source": [
    "Prompt the base model and examine its response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08acaf0-53c8-439d-a30d-939ee43f9283",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the feedback, we know:\n",
      "- C is in the correct position (1st letter)\n",
      "- R is in the correct position (2nd letter)\n",
      "- A is in the correct position (3rd letter)\n",
      "- N is not in the word\n",
      "- E is not in the word\n",
      "- S is not in the word\n",
      "- H is not in the word\n",
      "\n",
      "This leaves us with the letters B, D, G, I, J, K, L, M, O, P, Q, T, U, V, W, X, Y, Z to choose from for the 4th and 5th positions. We should try to include letters that are common in 5-letter words and haven't been ruled out yet.</think>\n",
      "<guess>CRANE</guess>\n"
     ]
    }
   ],
   "source": [
    "base_completion = generate_stream(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bd44d",
   "metadata": {},
   "source": [
    "Now prompt the RFT model and see how it responds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f6b1c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> <b>NOTE:</b> The cell below loads a model that has been tuned using RFT to play wordle. You'll learn how this model was trained throughout the course, and see the exact setup used to train the model on the Predibase platform in L8.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89de204-6e6f-4c85-93cb-98f4003f58b4",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'api_token': '', 'object': 'error', 'message': 'The model `wordle-dlai/2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prompt fine-tuned model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ft_completion \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwordle-dlai/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgenerate_stream\u001b[0;34m(prompt, adapter_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_stream\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, adapter_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Streams a model-generated response from a prompt in \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    real-time and prints it as it arrives.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Produce deterministic responses for evaluation\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/completions.py:545\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1276\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1264\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1272\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1273\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1274\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:949\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1056\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1060\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1061\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1066\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'api_token': '', 'object': 'error', 'message': 'The model `wordle-dlai/2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}"
     ]
    }
   ],
   "source": [
    "# prompt fine-tuned model\n",
    "ft_completion = generate_stream(prompt, adapter_id=\"wordle-dlai/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2ef51",
   "metadata": {},
   "source": [
    "## Playing a game of wordle\n",
    "\n",
    "Start by setting up a helper function that gets feedback on a guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bb11f-d1f9-46dd-a3b8-e54ce34e49d7",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_feedback(guess: str, secret_word: str) -> List[LetterFeedback]:\n",
    "    valid_letters = set(secret_word)\n",
    "    feedback = []\n",
    "    for letter, secret_letter in zip(guess, secret_word):\n",
    "        if letter == secret_letter:\n",
    "            feedback.append(LetterFeedback.CORRECT)\n",
    "        elif letter in valid_letters:\n",
    "            feedback.append(LetterFeedback.WRONG_POS)\n",
    "        else:\n",
    "            feedback.append(LetterFeedback.WRONG_LETTER)\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8fff9",
   "metadata": {},
   "source": [
    "Now create a `next_turn` function that incorporates feedback on a guess into the prompt to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c755edc-3484-4f82-bf57-84f7aff51e5e",
   "metadata": {
    "height": 438
   },
   "outputs": [],
   "source": [
    "def next_turn(\n",
    "    past_guesses: List[GuessWithFeedback], \n",
    "    secret_word: str, \n",
    "    adapter_id = \"\"\n",
    "):\n",
    "    prompt = render_prompt(past_guesses)\n",
    "    completion = generate_stream(prompt, adapter_id)\n",
    "    match = re.search(\n",
    "        r\"<guess>\\s*(.*?)\\s*</guess>\", completion, re.DOTALL\n",
    "    )\n",
    "    if not match:\n",
    "        raise RuntimeError(\"invalid guess\")\n",
    "    \n",
    "    guess = match.group(1).upper()\n",
    "    feedback = get_feedback(guess, secret_word)\n",
    "    past_guesses.append(GuessWithFeedback(guess, feedback))\n",
    "    print(\"\\n\\n\")\n",
    "    print((\"-\" * 100) + \"\\n\")\n",
    "    for past_guess in past_guesses:\n",
    "        print(past_guess)\n",
    "    \n",
    "    if guess == secret_word:\n",
    "        print(\"🎉 SUCCESS 🎉\")\n",
    "    elif len(past_guesses) >= 6:\n",
    "        print(\"❌ better luck next time... ❌\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22416f78",
   "metadata": {},
   "source": [
    "Try playing with the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2abc6-2099-489c-83ec-d8cae1f872c6",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "secret_word = \"BRICK\"\n",
    "past_guesses = []\n",
    "adapter_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412db7c-5e7c-4ec7-ad80-78ef86155187",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d2243",
   "metadata": {},
   "source": [
    "Now try with the finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f408fe2-f212-4fc7-8372-863d488220c1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "secret_word = \"BRICK\"\n",
    "past_guesses = []\n",
    "adapter_id = \"wordle-dlai/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c187f2-b4b4-4127-bff6-df8c466236de",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_turn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnext_turn\u001b[49m(past_guesses, secret_word, adapter_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_turn' is not defined"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a89f6",
   "metadata": {},
   "source": [
    "## Try for yourself!\n",
    "\n",
    "Try different secret words above and see how the model responds. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
